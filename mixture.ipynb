{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13f8f69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.config import RESULTS_FOLDER, get_config\n",
    "from file_utils import load_most_recent_results, load_most_recent_model\n",
    "from src.analysis import combine_all_trials, process_results, compute_top_codes\n",
    "import numpy as np\n",
    "from src.model import build_model\n",
    "import os\n",
    "import pickle as pk\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from src.data import get_mnist_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9277ce75",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dict = load_most_recent_results(RESULTS_FOLDER)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "159ff6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_trials = len(result_dict.keys())\n",
    "combined_results = combine_all_trials(result_dict)\n",
    "processed_result = process_results(combined_results)\n",
    "top_codes = compute_top_codes(result_dict[0], NUM_TOP_CODES=5)\n",
    "top_full_codes_info = top_codes['post_train_code_histogram']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c897a03b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "{'top_codes': [(1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0),\n  (0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0),\n  (0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1),\n  (1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1),\n  (0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1)],\n 'cmass_top_code': [0.024166666666666666,\n  0.037983333333333334,\n  0.04883333333333333,\n  0.059283333333333334,\n  0.06866666666666667],\n 'mass_top_code': [0.024166666666666666,\n  0.013816666666666666,\n  0.01085,\n  0.01045,\n  0.009383333333333334],\n 'ratio_1_0': 1.5075094684602324}"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_full_codes_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "823c965a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "[(1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0),\n (0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0),\n (0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1),\n (1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1),\n (0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1)]"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_codes = top_full_codes_info['top_codes']\n",
    "top_codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d264c5af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "OrderedDict([('first_layer.weight',\n              tensor([[-1.6704e-03, -3.1063e-02,  1.9776e-02,  ..., -1.4933e-02,\n                        4.6959e-03, -3.4221e-02],\n                      [-4.6716e-02, -2.8994e-02, -1.0641e-02,  ..., -6.4256e-06,\n                       -3.5220e-02,  7.9628e-03],\n                      [-9.1903e-03,  1.7133e-02, -1.3676e-02,  ...,  9.5499e-03,\n                        5.9822e-03, -4.8659e-02],\n                      ...,\n                      [-2.1377e-02,  1.8013e-02, -2.2198e-02,  ..., -3.8224e-02,\n                        5.3193e-03,  1.7176e-02],\n                      [-1.1148e-02, -1.2302e-02,  2.8995e-02,  ..., -9.5972e-03,\n                       -8.3732e-03, -3.3763e-02],\n                      [ 1.0330e-03, -3.5355e-02, -1.4247e-02,  ..., -4.8571e-02,\n                       -4.3420e-02, -5.4056e-02]])),\n             ('first_layer.bias',\n              tensor([ 0.0307,  0.0531,  0.0157,  0.0181, -0.0228,  0.0394,  0.0319,  0.0411])),\n             ('list_layers.0.weight',\n              tensor([[-0.3475,  0.2777,  0.4942,  0.2165, -0.7229,  0.4014, -0.3137,  0.4442],\n                      [ 0.1118,  0.4974,  0.0796, -0.5598,  0.6264,  0.4976,  0.1604, -0.1208],\n                      [ 0.2628, -0.2429, -0.2480,  0.4093,  0.1634,  0.9780, -0.7068, -0.1747],\n                      [-0.0024, -0.5476, -0.3541,  0.5174, -0.3416,  0.4645,  0.4974, -0.0314],\n                      [-0.1939,  0.1265,  0.0024,  0.4571,  0.2911, -0.8038,  0.3643,  0.2078],\n                      [ 0.7768,  0.1387,  0.3767,  0.2663, -0.1429, -0.0960, -0.4372, -0.5816],\n                      [ 0.6558, -0.2715,  0.3582, -0.6747,  0.4714, -0.0319,  0.3038,  0.5697],\n                      [-0.3273, -0.1130,  1.1600,  0.2194,  0.4068, -0.1010,  0.0591, -0.2673]])),\n             ('list_layers.0.bias',\n              tensor([ 0.2202,  0.0167,  0.4272,  0.0193, -0.1238, -0.3798,  0.1667, -0.0236])),\n             ('last_layer.weight',\n              tensor([[ 0.8859, -0.0261, -0.6356, -0.1804,  0.1179, -0.1724,  0.3463, -1.0540],\n                      [-1.0870, -0.2466,  0.4880,  0.7036, -0.2603, -0.4586, -0.2173,  0.6609],\n                      [ 0.5863,  0.3489, -0.0284,  0.5779, -0.2804,  0.0679,  0.1079, -0.3928],\n                      [ 0.3076, -0.2483,  0.2210,  0.1913, -0.4987,  0.2193, -0.0668,  0.4736],\n                      [-0.1441,  0.0188, -0.9290, -0.1986,  0.2003,  0.8738, -0.3834, -0.1330],\n                      [ 0.0607, -0.1754,  0.3754, -0.3590,  0.4740, -0.1450,  0.3368,  0.1939],\n                      [-0.2353,  0.4990, -0.6921, -0.0514,  0.4335, -0.6733, -0.2777, -0.3579],\n                      [-0.1565, -0.3796,  0.4270,  0.2954, -0.4520,  0.4114,  0.5761, -0.4780],\n                      [ 0.3883, -0.3050, -0.0145,  0.3393,  0.3916,  0.0697, -0.5555,  0.2213],\n                      [ 0.0735, -0.7533,  0.2318, -0.5612,  0.1985,  0.7911, -0.2762,  0.0300]])),\n             ('last_layer.bias',\n              tensor([-0.1357,  0.1111, -0.6150, -0.2080, -0.1141, -0.1671, -0.1250, -0.0336,\n                      -0.2686,  0.6424]))])"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = load_most_recent_model(RESULTS_FOLDER)\n",
    "model.eval()\n",
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d9d3c37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splits a single code into a list of code for each layer.\n",
    "def tuple_code_to_list(full_code):\n",
    "    hidden_layer_size = get_config()['hidden_size']\n",
    "    depth = get_config()['depth']\n",
    "    list_codes_per_layer=  []\n",
    "    for i in range(depth):\n",
    "        start_idx = i * hidden_layer_size\n",
    "        end_idx = start_idx + hidden_layer_size\n",
    "        layer_code = full_code[start_idx : end_idx]\n",
    "        list_codes_per_layer.append(list(layer_code))\n",
    "    print(full_code, '->',list_codes_per_layer )\n",
    "    return list_codes_per_layer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c2e59b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0) -> [[1, 0, 1, 1, 0, 1, 0, 1], [1, 0, 1, 1, 0, 1, 1, 0]]\n"
     ]
    }
   ],
   "source": [
    "test = tuple_code_to_list(top_codes[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f9b1b450",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearSoftMax(torch.nn.Module):\n",
    "    def __init__(self, input_size, weights, bias, output_size = 10):\n",
    "        super(LinearSoftMax, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.weights = torch.from_numpy(weights)\n",
    "        self.bias = torch.from_numpy(bias)\n",
    "        self.linear = torch.nn.Linear(self.input_size, output_size)\n",
    "        # nn.Parameter(F.softmax(self.layer_weights,dim=0))\n",
    "        with torch.no_grad():\n",
    "            self.linear.weight = torch.nn.Parameter(self.weights)\n",
    "            self.linear.bias = torch.nn.Parameter(self.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, self.input_size)\n",
    "        x = self.linear(x)\n",
    "        log_out = F.log_softmax(x, dim=1)\n",
    "        return log_out\n",
    "\n",
    "# Accepts a relu activation code and generates a linear softmax model from it.\n",
    "def NN_to_logreg(model, list_codes_per_layer):\n",
    "    model.eval()\n",
    "    weights = []\n",
    "    biases = []\n",
    "    for i, layer_code in enumerate(list_codes_per_layer):\n",
    "        # build matrix to cancel off ReLU's\n",
    "        layer = model.first_layer if i == 0 else model.list_layers[i - 1] # fix this so we can access the layers directly in model.list_layer\n",
    "        cancel_matrix = np.eye(layer.weight.shape[0])\n",
    "        for r in range(len(layer_code)):\n",
    "            cancel_matrix[r, r] = layer_code[r]\n",
    "        new_weight = np.matmul(cancel_matrix, layer.weight.detach().numpy())\n",
    "        new_bias = np.matmul(cancel_matrix, layer.bias.detach().numpy())\n",
    "        weights.append(new_weight)\n",
    "        biases.append(new_bias)\n",
    "    # Add last layer fed into softmax\n",
    "    weights.append(model.last_layer.weight.detach().numpy())\n",
    "    biases.append(model.last_layer.bias.detach().numpy())\n",
    "\n",
    "    # Combine all weights and biases into a single\n",
    "    combined_weight = weights[len(weights) - 1]\n",
    "    combined_bias = biases[len(biases) - 1]\n",
    "    for i in range(len(weights) - 2, -1, -1):\n",
    "        combined_bias = combined_bias + np.matmul(combined_weight, biases[i]) # This line should go before the below one\n",
    "        combined_weight = np.matmul(combined_weight, weights[i])\n",
    "    return LinearSoftMax(model.input_size, combined_weight, combined_bias)\n",
    "\n",
    "\n",
    "linearized = NN_to_logreg(model, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "871879c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28.48\n"
     ]
    }
   ],
   "source": [
    "train_loader, test_loader = get_mnist_data(get_config())\n",
    "\n",
    "correct_preds_count = 0\n",
    "for X, y in test_loader:\n",
    "    X_double = X.double()\n",
    "    output = linearized(X_double)\n",
    "    model_output, codes = model.forward_get_code(X)\n",
    "    layer_1 = codes[0].detach().numpy().astype(int)\n",
    "    layer_2 = codes[1].detach().numpy().astype(int)\n",
    "    zipped = list(zip(layer_1, layer_2))\n",
    "    combined_codes = [tuple(code[0]) + tuple(code[1]) for code in zipped]\n",
    "    indices_for_matching_code = [i for i in range(len(combined_codes)) if combined_codes[i] == top_codes[0]]\n",
    "    pred = [int(out.cpu().detach().numpy())\n",
    "            for out in output.data.max(1, keepdim=True)[1]]\n",
    "    model_pred = [int(out.cpu().detach().numpy())\n",
    "                  for out in model_output.data.max(1, keepdim=True)[1]]\n",
    "    correct_preds = (pred == y.detach().numpy())\n",
    "    correct_preds_count += correct_preds.sum()\n",
    "\n",
    "\n",
    "accuracy = 100. * correct_preds_count / len(test_loader.dataset)\n",
    "print(accuracy)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
